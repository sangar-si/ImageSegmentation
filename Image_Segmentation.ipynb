{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11eVGcPYX-XcyZdtDtDYOQDEmEbnLgO7b",
      "authorship_tag": "ABX9TyNwp2w+OCWuk3n7QjW8h7wi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qH9jRdz440N"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/Data/kaggle.json /root/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c carvana-image-masking-challenge\n",
        "!cp /content/drive/MyDrive/Data/extract.sh /content/extract.sh\n",
        "!sh extract.sh"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD, RMSprop\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from os import listdir\n",
        "from os.path import splitext\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "mTv8rdhJJdAx"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('metadata.csv')"
      ],
      "metadata": {
        "id": "eIOwcDe2MWY0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = \"/content/train\"\n",
        "mask_dir = \"/content/train_masks\"\n",
        "img_path = Path(images_dir)\n",
        "\n",
        "class LoadData(Dataset):\n",
        "  def __init__(self,img_path, mask_path, scale = 1):\n",
        "    self.img_path = Path(img_path)\n",
        "    self.mask_path = Path(mask_path)\n",
        "    self.scale = scale\n",
        "    self.img_id = [splitext(f)[0] for f in listdir(img_path) if not f.startswith(\".\")]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_id)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    name = self.img_id[idx]\n",
        "    mask_file = list(self.mask_path.glob(name +\"_mask\" +'.*'))\n",
        "    img_file = list(self.img_path.glob(name+'.*'))\n",
        "    mask = self.load(mask_file[0])\n",
        "    img = self.load(img_file[0])\n",
        "    mask = self.preprocess(img = mask, scale = self.scale, is_mask = True)\n",
        "    img = self.preprocess(img = img, scale = self.scale, is_mask = False)\n",
        "\n",
        "    return {\n",
        "        'image': torch.as_tensor(img.copy()).float().contiguous(),\n",
        "        'mask' : torch.as_tensor(mask.copy()).long().contiguous()\n",
        "    }\n",
        "  @classmethod\n",
        "  def load(cls,filename):\n",
        "    return Image.open(filename)\n",
        "\n",
        "  @classmethod\n",
        "  def preprocess(cls,img,scale,is_mask):\n",
        "    w, h = img.size\n",
        "    new_w, new_h = int(w * scale), int(h * scale)\n",
        "    img = img.resize((new_w, new_h), resample=Image.NEAREST if is_mask else Image.BICUBIC)\n",
        "    img_arr = np.asarray(img)\n",
        "    if img_arr.ndim == 2 and not is_mask:\n",
        "      img_arr = img_arr[np.newaxis, ...]\n",
        "    elif not is_mask:\n",
        "      img_Arr = img_arr.transpose((2,0,1))\n",
        "    if not is_mask:\n",
        "      img_arr = img_arr / 255\n",
        "    return img_arr\n"
      ],
      "metadata": {
        "id": "jSgYxxoc2qDQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = LoadData(img_path = images_dir, scale = 0.5, mask_path= mask_dir)"
      ],
      "metadata": {
        "id": "ylSH_w1W3AfM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.matshow(np.asarray(ds[1]['mask']))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "P6qyIFhFpbK4",
        "outputId": "b44bdda6-ffcc-434e-c8b9-e79c50c91f0e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 431.55x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "\n",
        "class EnBlock(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch):\n",
        "    super().__init__()\n",
        "    self.convBlock = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3), nn.ReLU(), nn.Conv2d(out_ch, out_ch, 3), nn.ReLU())\n",
        "  def forward(self, x):\n",
        "    return self.convBlock(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, chnls = (3,64,128,256,512,1024)):\n",
        "    super().__init__()\n",
        "    self.en_blocks = nn.ModuleList([EnBlock(chnls[i], chnls[i+1]) for i in range(len(chnls)-1)])\n",
        "    self.pool = nn.MaxPool2d(2)\n",
        "\n",
        "  def forward(self,x):\n",
        "    enc_features = []\n",
        "    for block in self.en_blocks:\n",
        "      x = block(x)\n",
        "      enc_features.append(x)\n",
        "      x = self.pool(x)\n",
        "    return enc_features\n",
        "\n",
        "class DeBlock(nn.Module):\n",
        "  def __init__(self, in_ch, out_ch):\n",
        "    super().__init__()\n",
        "    self.upConv = nn.ConvTranspose2d(in_ch, out_ch, 2, 2)\n",
        "    self.decBlock = EnBlock(in_ch, out_ch)\n",
        "\n",
        "  def crop(self, enc_features, x):\n",
        "    _,_,h,w = x.shape\n",
        "    enc_features   = torchvision.transforms.CenterCrop([h, w])(enc_features) \n",
        "    return enc_features\n",
        "\n",
        "  def forward(self, x, enc_features):\n",
        "    x = self.upConv(x)\n",
        "    enc_f = self.crop(enc_features,x)\n",
        "    x = torch.cat([x, enc_f], dim=1)\n",
        "    x = self.decBlock(x)\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, chnls = (1024, 512, 256, 128, 64)):\n",
        "    super().__init__()\n",
        "    self.chnls = chnls\n",
        "    self.de_blocks = nn.ModuleList([DeBlock(chnls[i], chnls[i+1]) for i in range(len(chnls)-1)])\n",
        "\n",
        "  def forward(self,x,enc_features):\n",
        "    for i in range(len(self.chnls)-1):\n",
        "      x = self.de_blocks[i](x, enc_features[i])\n",
        "    return x\n",
        "\n",
        "class UNet(nn.Module):\n",
        "  def __init__(self, enc_chnls = (3,64,128,256,512,1024), dec_chnls = (1024, 512, 256, 128, 64), num_class = 1, retain_dim=False, out_sz=(572,572)):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(enc_chnls)\n",
        "    self.decoder = Decoder(dec_chnls)\n",
        "    self.head = nn.Conv2d(dec_chnls[-1],num_class, 1)\n",
        "    self.retain_dim = retain_dim\n",
        "    self.out_sz = out_sz\n",
        "  \n",
        "  def forward(self,x):\n",
        "    enc_features = self.encoder(x)\n",
        "    out = self.decoder(enc_features[::-1][0], enc_features[::-1][1:])\n",
        "    out = self.head(out)\n",
        "    if self.retain_dim:\n",
        "      out = F.interpolate(out, self.out_sz)\n",
        "    return out\n",
        "\n",
        "class CEDiceLoss(nn.Module):\n",
        "  def __init__(self, weights = None, size_average = True):\n",
        "    super(CEDiceLoss,self).__init__()\n",
        "\n",
        "  def forward(self, mask_pred, mask_true, smooth = 1):\n",
        "    CELoss = nn.CrossEntropyLoss(mask_pred, mask_true)\n",
        "    mask_pred = F.softmax(mask_pred)\n",
        "    mask_pred = mask_pred.view(-1)\n",
        "    mask_true = mask_true.view(-1)\n",
        "    intersection = (mask_pred * mask_true).sum()\n",
        "    dice_loss = 1 - (2 * intersection + smooth) / (mask_pred.sum() + mask_true.sum() + smooth)\n",
        "    return CELoss+dice_loss\n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "id": "yyRUHLwvukPJ"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, epochs, batch_size, lr, device, scale, val_percent, amp):\n",
        "  images_dir = \"/content/train\"\n",
        "  mask_dir = \"/content/train_masks\"\n",
        "  img_path = Path(images_dir)\n",
        "  ds = LoadData(img_path = images_dir, scale = scale, mask_path= mask_dir)\n",
        "\n",
        "  n_val = int(len(ds) * val_percent)\n",
        "  n_train = len(ds) - n_val\n",
        "  train_set, val_set = random_split(ds, [n_train, n_val], generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "  loader_args = dict(batch_size = batch_size, num_workers = 4, pin_memory = True)\n",
        "  train_loader = DataLoader(train_set, shuffle=True, **loader_args)\n",
        "  val_loader = DataLoader(val_set, shuffle=False, drop_last = True, **loader_args) \n",
        "\n",
        "  optimizer = RMSProp(net.parameters(), lr = lr, weight_decay = 1e-8, momentum = 0.9)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)  # goal: maximize Dice score\n",
        "  grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  global_step = 0\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    net.train()\n",
        "    with tqdm(total = n_train, desc = f'Epoch {epoch + 1}/{epochs}', unit = 'img') as pbar:\n",
        "      for batch in train_loader:\n",
        "        imgs = batch['image']\n",
        "        masks = batch['mask']\n",
        "        imgs = imgs.to(device=device, dtype = torch.float32)\n",
        "        masks = masks.to(device=device, dtype = long)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled = amp):\n",
        "          mask_pred = net(imgs)\n",
        "          loss = criterion(mask_pred, mask)\\\n",
        "                           + dice_loss(F.softmax(masks_pred, dim=1).float(),\n",
        "                                       F.one_hot(true_masks, net.n_classes).permute(0, 3, 1, 2).float(),\n",
        "                                       multiclass=True)\n",
        "        optimizer.zero_grad(set_to_none = True)\n",
        "        grad_scaler.scale(loss).backward()\n",
        "        grad_scaler.step(optimizer)\n",
        "        grad_scaler.update()\n",
        "\n",
        "        pbar.update(imgs.shape[0])\n",
        "        global_step +=1\n",
        "        epoch_loss += loss.item()\n",
        "        pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
        "\n",
        "        #Eval\n",
        "        div_step = (n_train//(10*batch_size))\n",
        "        if div_step > 0:\n",
        "          if global_step % div_step == 0:\n",
        "            val_score = evaluate(net, val_loader, device)\n",
        "            scheduler.step(val_score)\n",
        "    if save_checkpoint:\n",
        "      try:\n",
        "        torch.save(net.state_dict(),\"unet\"+str(epoch)+\".pth\")\n",
        "      except:\n",
        "        print(\"Save failed\")"
      ],
      "metadata": {
        "id": "8kt4vXAZYPVj"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = UNet(num_class=2)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net.to(device = device)\n",
        "train(net=net, epochs = 3, batch_size = 10, lr = 0.00001, device = device, scale = 0.5, val_percent = 10/100, amp = False)\n"
      ],
      "metadata": {
        "id": "SvaDIzAx6IO6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}